<!DOCTYPE html>
<html>
  <head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
  <meta name="description" content="jayandjean&#39;s blog">
  <meta name="keyword" content="hexo-theme, vuejs">
  
    <link rel="shortcut icon" href="/css/images/logo.png">
  
  <title>
    
      IV.机器学习模型-深度模型中的优化 | JAYANDJEAN
    
  </title>
  <link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.css" rel="stylesheet">
  <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/tomorrow.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/plugins/gitment.css">
  <script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
  <script src="//cdn.bootcss.com/geopattern/1.2.3/js/geopattern.min.js"></script>
  <script src="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.js"></script>
  <script src="/js/qrious.js"></script>
<script src="/js/gitment.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="header fixed-header">
  <div class="header-container">
    <a class="home-link" href="/">
      <div class="logo"></div>
      <span>JAYANDJEAN</span>
    </a>
    <ul class="right-list">
      
        <li class="list-item">
          
            <a href="/" class="item-link">Home</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/tags/" class="item-link">Tags</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/archives/" class="item-link">Archives</a>
          
        </li>
      
    </ul>
    <div class="menu">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </div>
    <div class="menu-mask">
      <ul class="menu-list">
        
          <li class="menu-item">
            
              <a href="/" class="menu-link">Home</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/tags/" class="menu-link">Tags</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/archives/" class="menu-link">Archives</a>
            
          </li>
        
      </ul>
    </div>
  </div>
</header>

    <div id="article-banner">
  <h2>IV.机器学习模型-深度模型中的优化</h2>
  <p class="post-date">2018-04-14</p>
  <div class="arrow-down">
    <a href="javascript:;"></a>
  </div>
</div>
<main class="app-body">
  <article class="post-article">
    <section class="markdown-content"><p>$\textit{Deep Learning}$ 第八章读书笔记。</p>
<p>未完成。</p>
<a id="more"></a>
<h2 id="8-1-学习和纯优化有什么不同"><a href="#8-1-学习和纯优化有什么不同" class="headerlink" title="8.1 学习和纯优化有什么不同"></a>8.1 学习和纯优化有什么不同</h2><h3 id="8-1-1-经验风险最小化"><a href="#8-1-1-经验风险最小化" class="headerlink" title="8.1.1 经验风险最小化"></a>8.1.1 经验风险最小化</h3><p>区分生成数据的真实分布$p_{data}$和经验分布$\hat{p}_{data}$。</p>
<h3 id="8-1-2-代理损失函数和提前终止"><a href="#8-1-2-代理损失函数和提前终止" class="headerlink" title="8.1.2 代理损失函数和提前终止"></a>8.1.2 代理损失函数和提前终止</h3><p>有些损失函数在数学上计算梯度较为复杂，则可以考虑换一种函数形式，即代理损失函数。</p>
<h3 id="8-1-3-批量算法和小批量算法"><a href="#8-1-3-批量算法和小批量算法" class="headerlink" title="8.1.3 批量算法和小批量算法"></a>8.1.3 批量算法和小批量算法</h3><p>区分批量算法、随机算法和小批量算法。</p>
<h2 id="8-2-神经网络优化中的挑战"><a href="#8-2-神经网络优化中的挑战" class="headerlink" title="8.2 神经网络优化中的挑战"></a>8.2 神经网络优化中的挑战</h2><p>未看！</p>
<h2 id="8-3-基本算法"><a href="#8-3-基本算法" class="headerlink" title="8.3 基本算法"></a>8.3 基本算法</h2><p>这块内容主要摘抄自知乎的文章，这些公式多抄几遍可能就看懂了，同时也修正了部分公式中的小错误。</p>
<h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>梯度下降算法是指，在给定待优化的模型参数 $\theta \in \mathbb{R}^d$ 和目标函数 $J(\theta)$ 后，算法通过沿梯度 $\nabla_\theta J(\theta)$ 的相反方向更新 $\theta$ 来最小化 $J(\theta)$ 。学习率 $\eta$ 决定了每一时刻的更新步长。对于每一个时刻 $t$ ，我们可以用下述步骤描述梯度下降的流程：</p>
<ol>
<li>计算目标函数关于参数的梯度：$$g_t = \nabla_\theta J(\theta)$$</li>
<li>根据历史梯度计算一阶和二阶动量：$$m_t = \phi(g_1, g_2, \cdots, g_t) $$$$v_t = \psi(g_1, g_2, \cdots, g_t) $$</li>
<li>更新模型参数：$$\theta_{t+1} = \theta_t - \frac{m_t}{\sqrt{v_t} + \epsilon}$$其中， $\epsilon$ 为平滑项，防止分母为零，通常取 1e-8。</li>
</ol>
<h3 id="SGD-Stochastic-Gradient-Descent"><a href="#SGD-Stochastic-Gradient-Descent" class="headerlink" title="SGD (Stochastic Gradient Descent)"></a>SGD (Stochastic Gradient Descent)</h3><p>随机梯度下降算法最为简单，没有动量的概念，即：$$m_t = \eta \cdot g_t $$$$v_t = I^2 $$$$\epsilon = 0 $$</p>
<p>这时，更新步骤就是最简单的：$$\theta_{t+1}= \theta_t - \eta \cdot g_t $$<br>SGD 的缺点在于收敛速度慢，可能在鞍点处震荡。并且，如何合理的选择学习率是 SGD 的一大难点。在实践中，有必要随着时间的推移逐渐降低学习率，因此我们将第 $t$ 步迭代的学习率记作 $\eta_t$。保证 SGD 收敛的一个充分条件是：$$\sum_{t=1}^{\infty}\eta_t=\infty;\sum_{t=1}^{\infty}\eta_{t}^2&lt;\infty$$<br>实践中，一般会线性衰减学习率直到第 $\tau$ 次迭代：$$\eta_t = (1 − \alpha)\cdot\eta_0 + \alpha\cdot\eta_{\tau}$$其中 $\alpha=\frac{t}{\tau}$。在 $\tau$ 步迭代之后，一般使 $\eta$ 保持常数。</p>
<h3 id="SGD-with-Momentum"><a href="#SGD-with-Momentum" class="headerlink" title="SGD with Momentum"></a>SGD with Momentum</h3><p>为了抑制 SGD 的震荡，SGDM 认为梯度下降过程可以加入惯性。下坡的时候，如果发现是陡坡，那就利用惯性跑的快一些。SGDM是在SGD基础上引入了一阶动量：<br>$$m_t = \gamma \cdot m_{t-1} + \eta \cdot g_t $$</p>
<p>这时，更新步骤是：$$\theta_{t+1}= \theta_t - m_t $$</p>
<p>一阶动量是各个时刻梯度方向的累积量。</p>
<p>也就是说，$t$ 时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。 $\gamma$ 的经验值为0.9，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。</p>
<h3 id="Nesterov-Accelerated-Gradient"><a href="#Nesterov-Accelerated-Gradient" class="headerlink" title="Nesterov Accelerated Gradient"></a>Nesterov Accelerated Gradient</h3><p>SGD 还有一个问题是困在局部最优的沟壑里面震荡。NAG全称Nesterov  Accelerated Gradient，是在SGD、SGDM的基础上的进一步改进，改进点在于计算 $g_t$。我们知道在时刻$t$的主要下降方向是由累积动量决定的，自己的梯度方向说了也不算，那与其看当前梯度方向，不如先看看如果跟着累积动量走了一步，那个时候再怎么走。因此，NAG在计算 $g_t$，不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的下降方向：$$g_t = \nabla_\theta J(\theta - \gamma \cdot m_{t-1})$$$$m_t = \gamma \cdot m_{t-1} + \eta \cdot g_t $$</p>
<p>这时，更新步骤是：$$\theta_{t+1}= \theta_t - m_t $$</p>
<p>可用下图表示，长红色向量与长绿色向量平行，均为累积动量。也就是从 SGDM 的角度来说，长红色向量为累积动量，短红色向量为当前计算出的梯度向量，那么长蓝色向量即为参数前进的方向。而从 NAG 的角度来说，参数先沿着长绿色向量走，并计算该处的梯度向量，即短红色向量，并与累积动量相加，即长灰色向量，也就是最后参数前进的方向。<br><img src="http://7xs8n3.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-14%20%E4%B8%8B%E5%8D%8810.56.32.png" alt=""></p>
<h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p>此前我们都没有用到二阶动量。二阶动量的出现，才意味着“自适应学习率”优化算法时代的到来。</p>
<p>$$v_t=\sum_{\tau=1}^{t}g_{\tau}^2$$$$m_t=\eta \cdot g_t$$</p>
<p>这时，更新步骤是：$$\theta_{t+1}= \theta_t - \frac{\eta \cdot g_t}{\sqrt{v_t}+\epsilon} $$</p>
<p>从训练开始时积累梯度平方会导致有效学习率过早和过量的减小。AdaGrad在某些深度学习模型上效果不错，但不是全部。</p>
<h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p>同样借鉴Momentum的想法，由AdaGrad可以得到RMSprop算法：$$v_t=\gamma \cdot v_{t-1} + (1-\gamma) \cdot g_t^2$$$$m_t=\eta \cdot g_t$$这时，更新步骤是：$$\theta_{t+1}= \theta_t - \frac{\eta \cdot g_t}{\sqrt{v_t}+\epsilon} $$</p>
<p>$\gamma$ 通常取 0.9 左右。</p>
<h3 id="AdaDelta"><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a>AdaDelta</h3><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>在一阶动量和二阶动量上均借鉴Momentum的想法，可以得到Adam算法：</p>
<p>$$m_t = \eta \cdot \left[ \beta_1 m_{t-1} + (1 - \beta_1)g_t \right] $$$$v_t = \beta_2 \cdot v_{t-1} + (1-\beta_2) \cdot g_t^2 $$</p>
<p>实际使用过程中，参数的经验值是：$$\beta_1=0.9, \beta_2=0.999$$</p>
<p>$$m_t = \frac{m_t}{1-\beta_1^t} $$$$v_t = \frac{v_t}{1-\beta_2^t} $$</p>
<p>再进行更新，</p>
<p>$$\theta_{t+1} = \theta_t - \frac{m_t}{\sqrt{v_t} + \epsilon } $$</p>
<h3 id="Nadam"><a href="#Nadam" class="headerlink" title="Nadam"></a>Nadam</h3><p>Nesterov + Adam = Nadam</p>
<h3 id="可视化对比"><a href="#可视化对比" class="headerlink" title="可视化对比"></a>可视化对比</h3><p><a href="https://github.com/JAYANDJEAN/optimizer_with_tf/blob/master/optimizer_visual.py" target="_blank" rel="external">visual_code</a></p>
<p><img src="http://7xs8n3.com1.z0.glb.clouddn.com/v2-5d5166a3d3712e7c03af74b1ccacbeac_b.gif" alt=""></p>
<p><img src="http://7xs8n3.com1.z0.glb.clouddn.com/v2-4a3b4a39ab8e5c556359147b882b4788_b.gif" alt=""></p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li>$\textit{Deep Learning}$</li>
<li><a href="https://zhuanlan.zhihu.com/p/32626442" target="_blank" rel="external">从 SGD 到 Adam —— 深度学习优化算法概览(一)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32230623" target="_blank" rel="external">Adam那么棒，为什么还对SGD念念不忘(1)：一个框架看懂优化算法</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32262540" target="_blank" rel="external">Adam那么棒，为什么还对SGD念念不忘(2)：Adam的两宗罪</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32338983" target="_blank" rel="external">Adam那么棒，为什么还对SGD念念不忘(3)：优化算法的选择与使用策略</a></li>
</ul>
</section>
    
      <div class="tags">
        <span>Tags:</span>
        
  <a href="/tags#机器学习模型" >
    <span class="tag-code">机器学习模型</span>
  </a>

      </div>
    
    <div class="money-like">
      <div class="reward-btn">
        赏
        <span class="money-code">
          <span class="alipay-code">
            <div class="code-image"></div>
            <b>使用支付宝打赏</b>
          </span>
          <span class="wechat-code">
            <div class="code-image"></div>
            <b>使用微信打赏</b>
          </span>
        </span>
      </div>
      <p class="notice">欢迎打赏</p>
    </div>
    
  </article>
</main>

<script>
  (function () {
    var url = 'http://yoursite.com/2018/04/14/19_ML_Optimization/';
    var banner = ''
    if (banner) {
      $('#article-banner').css({
        'background-image': 'url(' + banner + ')'
      })
    } else {
      $('#article-banner').geopattern(url)
    }
    $('.header').removeClass('fixed-header')

     // error image
    $(".markdown-content img").on('error', function() {
      $(this).attr('src', 'http://file.muyutech.com/error-img.png') 
    })

    // zoom image
    $(".markdown-content img").on('click', function() {
      var src = $(this).attr('src')
      var imageW = $(this).width()
      var imageH = $(this).height()
      
      var zoom = ($(window).width() * 0.95 / imageW).toFixed(2)
      zoom = zoom < 1 ? 1 : zoom
      zoom = zoom > 2 ? 2 : zoom
      var transY = (($(window).height() - imageH) / 2).toFixed(2)

      $('body').append('<div class="image-view-wrap"><div class="image-view-inner"><img src="'+ src +'" /></div></div>')
      $('.image-view-wrap').addClass('wrap-active')
      $('.image-view-wrap img').css({
        'width': `${imageW}`,
        'transform': `translate3d(0, ${transY}px, 0) scale3d(${zoom}, ${zoom}, 1)`
      })
      $('html').css('overflow', 'hidden')

      $('.image-view-wrap').on('click', function() {
        $(this).remove()
        $('html').attr('style', '')
      })
    })

    // qrcode
    var qr = new QRious({
      element: document.getElementById('share-qrcode'),
      value: document.location.href
    });

    // gitment
    var gitmentConfig = "";
    if (gitmentConfig != "undefined") {
      var gitment = new Gitment({
        id: "IV.机器学习模型-深度模型中的优化",
        owner: "",
        repo: "",
        oauth: {
          client_id: "",
          client_secret: ""
        },
        theme: {
          render(state, instance) {
            const container = document.createElement('div')
            container.lang = "en-US"
            container.className = 'gitment-container gitment-root-container'
            container.appendChild(instance.renderHeader(state, instance))
            container.appendChild(instance.renderEditor(state, instance))
            container.appendChild(instance.renderComments(state, instance))
            container.appendChild(instance.renderFooter(state, instance))
            return container;
          }
        }
      })
      gitment.render(document.getElementById('comments'))
    }
  })();
</script>

    <div class="scroll-top">
  <span class="arrow-icon"></span>
</div>
    <footer class="app-footer">
  <p class="copyright">
    &copy; 2018 | Proudly powered by <a href="https://hexo.io" target="_blank">Hexo</a>
    <br>
    Theme by <a href="https://github.com/yanm1ng">yanm1ng</a>
  </p>
</footer>

<script>
  function async(u, c) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }
</script>
<script>
  async("//cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
    FastClick.attach(document.body);
  })
</script>

<script>
  var hasLine = 'true';
  async("//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js", function(){
    $('figure pre').each(function(i, block) {
      var figure = $(this).parents('figure');
      if (hasLine == 'false') {
        figure.find('.gutter').hide();
      }
      var lang = figure.attr('class').split(' ')[1] || 'code';
      var codeHtml = $(this).html();
      var codeTag = document.createElement('code');
      codeTag.className = lang;
      codeTag.innerHTML = codeHtml;
      $(this).attr('class', '').empty().html(codeTag);
      figure.attr('data-lang', lang.toUpperCase());
      hljs.highlightBlock(block);
    });
  })
</script>
<!-- Baidu Tongji -->

<script src="/js/script.js"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  </body>
</html>